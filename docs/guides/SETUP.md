# üõ†Ô∏è Guia de Instala√ß√£o - MangaAutoColor Pro v3.0

## Requisitos de Sistema

### Hardware (Validado)

| Componente | M√≠nimo | Recomendado | Validado |
|------------|--------|-------------|----------|
| GPU | NVIDIA GTX 1070 8GB | NVIDIA RTX 3060 12GB | RTX 3060 ‚úÖ |
| VRAM | 6 GB | 8+ GB | 8.0 GB (Peak) |
| RAM | 16 GB | 32 GB | - |
| CPU | 6 cores | 8+ cores | - |
| Armazenamento | 10 GB SSD | 50 GB NVMe | - |
| Internet | 10 Mbps | 50+ Mbps | - |

### Software (Validado)

- **OS**: Windows 10/11, Linux (Ubuntu 20.04+)
- **Python**: 3.10, 3.11 (recomendado)
- **CUDA**: 11.8, 12.1 (recomendado)
- **PyTorch**: 2.2.x, 2.3.x (compat√≠vel com CUDA 11.8/12.1)

> ‚ö†Ô∏è **Nota**: Use Python 3.10 ou 3.11 para melhor compatibilidade com xformers e insightface.

---

## üöÄ Instala√ß√£o R√°pida

### 1. Clone o Reposit√≥rio

```bash
git clone https://github.com/seu-usuario/manga-autocolor-pro.git
cd manga-autocolor-pro
```

### 2. Instala√ß√£o Autom√°tica (Windows)

Basta executar o script de instala√ß√£o, que cria o ambiente virtual, instala depend√™ncias e baixa os modelos:

```batch
scripts\windows\install.bat
```

### 3. Iniciar Servidor

```batch
scripts\windows\run.bat
```

---

## ‚úÖ Verifica√ß√£o Completa

### Smoke Test (Recomendado)

Execute o teste de integra√ß√£o que carrega modelos reais e valida o pipeline:

```bash
python scripts/smoke_test.py
```

**Sa√≠da esperada (RTX 3060):**
```
‚úÖ CUDA dispon√≠vel
‚úÖ Modelos baixados
‚úÖ SD 1.5 carregado
‚úÖ ControlNet carregado
‚úÖ IP-Adapter carregado
‚úÖ VAE configurado
‚úÖ Pipeline compilado
‚úÖ Teste de matem√°tica de tiles
‚úÖ Teste de convers√£o de cores
‚úÖ Pipeline de gera√ß√£o (1024x1024)

üéâ Todos os testes passaram!

‚ö° M√©tricas:
   - Tempo de infer√™ncia: ~25s (20 steps)
   - VRAM ap√≥s gera√ß√£o: 0.1GB
   - Status: OK
```

### Teste de Unidade

```bash
# Todos os testes
pytest tests/unit -v
```

---

## üì¶ Download de Modelos

### Download Autom√°tico

O `scripts\windows\install.bat` j√° executa o script de download. Se precisar rodar manualmente:

```bash
python scripts/download_models_v3.py
```

**Modelos V3 (~5 GB):**
| Modelo | Tamanho | Uso |
|--------|---------|-----|
| runwayml/stable-diffusion-v1-5 | ~2.5 GB | Base Model |
| lllyasviel/control_v11p_sd15s2_lineart_anime | ~1.4 GB | Lineart Control |
| h94/IP-Adapter (Plus Face SD15) | ~0.5 GB | Identidade |
| keremberke/yolov8m-manga-10k | ~50 MB | Detec√ß√£o |
| openai/clip-vit-large-patch14 | ~1.5 GB | Encoder |

---

## üîß Configura√ß√£o de Hardware

### RTX 3060 12GB (Configura√ß√£o Padr√£o)

O sistema j√° vem otimizado para RTX 3060:

```python
# config/settings.py (valores padr√£o)
DTYPE = torch.float16              # FP16 obrigat√≥rio
ENABLE_CPU_OFFLOAD = True          # Economia de VRAM
TILE_SIZE = 1024                   # Tamanho do tile (SD 1.5 nativo √© 512, mas suporta 1024 com tiling)
MAX_REF_PER_TILE = 2               # Limite de personagens
```

**Resultado medido:**
- VRAM Pico durante gera√ß√£o: **~5.5 GB** (CPU Offload)
- Tempo 1024√ó1408: **~25s**

### Outras GPUs

Para GPUs com mais VRAM (16GB+), desative CPU offload para mais velocidade:

```python
# RTX 4090 24GB
ENABLE_CPU_OFFLOAD = False
```

---

## üêõ Solu√ß√£o de Problemas

### Problema: `CUDA out of memory`

**Causa**: VRAM insuficiente ou modelos muito grandes.

**Solu√ß√£o:**
```python
# Em config/settings.py
ENABLE_CPU_OFFLOAD = True    # J√° ativado por padr√£o
TILE_SIZE = 768              # Reduzir tile
MAX_REF_PER_TILE = 1         # Limitar personagens
```

### Problema: `torch.cuda.OutOfMemoryError` durante download

**Causa**: Tentativa de carregar modelo em VRAM cheia.

**Solu√ß√£o:**
```bash
# Limpe VRAM
python -c "import torch; torch.cuda.empty_cache()"

# Ou reinicie o terminal
```

### Problema: `ModuleNotFoundError: No module named 'diffusers'`

**Solu√ß√£o:**
```bash
# Reinstale depend√™ncias
pip install -r requirements.txt --force-reinstall
```

### Problema: Modelos n√£o baixam (timeout)

**Solu√ß√£o:**
```bash
# Use mirror alternativo
export HF_ENDPOINT=https://hf-mirror.com

# Ou configure proxy
export HTTP_PROXY=http://proxy.company.com:8080
export HTTPS_PROXY=http://proxy.company.com:8080

# Download manual
python scripts/download_models.py --retry 5
```

### Problema: `insightface` n√£o instala no Windows

**Solu√ß√£o:**
```bash
# Insightface √© opcional (para ArcFace)
# Se falhar, o sistema usa apenas CLIP
pip install insightface --pre

# Ou ignore o erro - CLIP √© suficiente
```

### Problema: `RuntimeError: CUDA error: invalid device ordinal`

**Causa**: GPU n√£o detectada.

**Solu√ß√£o:**
```bash
# Verifique CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Se False, reinstale PyTorch com CUDA
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

---

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Vari√°veis de Ambiente

```bash
# Windows (PowerShell)
$env:CUDA_VISIBLE_DEVICES = "0"
$env:PYTORCH_CUDA_ALLOC_CONF = "max_split_size_mb:512"
$env:HF_HOME = "C:\Models\HuggingFace"
$env:HF_HUB_DISABLE_SYMLINKS = "1"

# Linux/Mac
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512"
export HF_HOME="/path/to/models"
```

### Configura√ß√£o por Arquivo

Crie `config/local.yaml` (opcional):

```yaml
performance:
  device: "cuda"
  dtype: "float16"
  enable_cpu_offload: true
  tile_size: 1024
  max_ref_per_tile: 2

models:
  sdxl_model: "ByteDance/SDXL-Lightning"
  sdxl_steps: 4
  controlnet: "diffusers/controlnet-canny-sdxl-1.0"
  yolo_model: "keremberke/yolov8m-manga-10k"

generation:
  ip_adapter_end_step: 0.6
  background_ip_scale: 0.0
  context_inflation: 1.5
```

---

## üéØ Execu√ß√£o

### Modo CLI

```bash
# Pipeline completo
python cli.py full ./chapter_01 --output ./output --style vibrant

# Apenas an√°lise
python cli.py analyze ./chapter_01

# Apenas gera√ß√£o
python cli.py generate --chapter-id <id> --pages 1,2,3
```

### Modo API

```bash
# Iniciar servidor
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

# Testar
curl http://localhost:8000/health
```

### Modo Python

```python
from core.pipeline import MangaColorizationPipeline

pipeline = MangaColorizationPipeline()
chapter_id, summary = pipeline.process_chapter("./chapter_01")

for result in pipeline.generate_chapter(chapter_id):
    result.image.save(f"output/page_{result.page_number:03d}.png")
```

---

## üìä Benchmark

```bash
python scripts/benchmark.py
```

**Resultados esperados:**

| Hardware | An√°lise | Gera√ß√£o 1024¬≤ | VRAM Pico |
|----------|---------|---------------|-----------|
| RTX 3060 12GB | ~2s/p√°gina | ~30s | ~11.5GB |
| RTX 4090 24GB | ~0.8s | ~8s | ~18GB |
| CPU (8 cores) | ~10s | ~300s | ~8GB RAM |

---

## üÜò Suporte

### Informa√ß√µes para Debug

Se precisar de ajuda, execute:

```bash
# Coleta informa√ß√µes do sistema
python -c "
import torch
import sys
print(f'Python: {sys.version}')
print(f'PyTorch: {torch.__version__}')
print(f'CUDA: {torch.version.cuda}')
print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')
print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB' if torch.cuda.is_available() else 'N/A')
"
```

### Logs de Erro

Habilite logs detalhados:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## üéâ Pr√≥ximos Passos

1. ‚úÖ **Execute o smoke test**: `python scripts/smoke_test.py`
2. üìö **Leia a [API Reference](API.md)**
3. üèóÔ∏è **Explore a [Arquitetura](ARCHITECTURE.md)**
4. üß™ **Execute testes**: `pytest tests/high/ -v`

**Bem-vindo ao MangaAutoColor Pro v2.0!** üé®
